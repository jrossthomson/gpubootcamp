{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "16July21 Tropical_Cyclone_Intensity_Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFR60TL0dUFB"
      },
      "source": [
        "# Welcome to HPC / AI workshop\n",
        "\n",
        "   <a target=\"_blank\" href=\"https://colab.research.google.com/github/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation.ipynb\"><img src=\"https://quantumai.google/site-assets/images/buttons/colab_logo_1x.png\" />Run this notebook in Google Colab</a>\n",
        "   \n",
        "\n",
        "## Technical Objectives\n",
        "\n",
        "The objective of this workshop is to give an introduction to application of Artificial Intelligence (AI) algorithms in scientific High Performance Computing (HPC) simulations. This workshop will introduce participants to fundamentals of AI and how those can be applied to different HPC simulation domains. \n",
        "\n",
        "## Getting started\n",
        "\n",
        "This notebook is easiest to run in Google Colab, but it will work on any Jupyter notebook environment where a GPU can be connected. \n",
        "\n",
        "### Running Colab\n",
        "\n",
        "If you want some extra help getting started with \n",
        "Google Colab, the following provides an introduction: \n",
        "[Welcome To Colaboratory](https://colab.sandbox.google.com/notebooks/intro.ipynb?utm_source=scs-index). \n",
        "However, if you are at all familiar with Jupyter Notebooks, the link below will get you started.\n",
        "\n",
        "   <a target=\"_blank\" href=\"https://colab.research.google.com/github/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation.ipynb\"><img src=\"https://quantumai.google/site-assets/images/buttons/colab_logo_1x.png\" />Run this notebook in Google Colab</a>\n",
        " \n",
        " ### Gmail account\n",
        " \n",
        "__Important__ : You will need a Gmail account to run the notebook on Golab and to save your notebook on Google Drive. [This is where you can create a Gmail account.](https://support.google.com/mail/answer/56256?hl=en)\n",
        "\n",
        "### CNN background\n",
        "\n",
        "If you are completely unfamiliar with Neural Networks (NN) or Machine Learning (ML) this tutorial can be a great introduction:\n",
        "\n",
        "- [CNN Primer and Keras 101](Intro_to_DL/Part_2.ipynb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjH7JVRpP-4Z"
      },
      "source": [
        "# Get the Git Repo\n",
        "**IMPORTANT** To get most of the data for this workshop, you need to clone the Git repo locally.\n",
        "\n",
        "Another piece of data is the partially trained model data in `trained_16.h5`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I0_6_14gdGI"
      },
      "source": [
        "!git clone https://github.com/jrossthomson/gpubootcamp.git\n",
        "!wget https://storage.googleapis.com/gpubootcamp_tcdata/trained_16.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUOrybpeewrz"
      },
      "source": [
        "# System check\n",
        "\n",
        "Before moving forward let us check if Tensorflow backend is able to see and use GPU There are a few things to check before proceding with the tutorial.\n",
        "\n",
        "* Do you have a GPU Running on the Colab\n",
        "* Which version of TensorFlow do you have runningU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJpWlWn2dUFD"
      },
      "source": [
        "# Import Necessary Libraries\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "print(tf.test.gpu_device_name())\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNcgF4QQdUFE"
      },
      "source": [
        "The output of the cell above will show all the available compaitable GPU on the system. If no GPU device is listed or you see an error means that there was no compaitable GPU present on the system and the future calls may run on CPU consuming more time\n",
        "\n",
        "### To enable GPUs in Colab\n",
        "\n",
        "On the Colab menu, \"Edit\" -> \"Notebook Settings\", a pop up will allow you to select a GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU14NLU_QpJa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# PART 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eKeu7ckdUFF"
      },
      "source": [
        "# Tropical cyclone intensity estimation using a Deep Convolutional Neural Network \n",
        "\n",
        "\n",
        "*Tropical cyclone intensity estimation is a challenging task as it requires domain knowledge while extracting\n",
        "features, significant pre-processing, various sets of parameters obtained from satellites, and human intervention for analysis.*\n",
        "\n",
        "*The inconsistency of results, significant pre-processing of data, the complexity of the problem domain, and problems on generalizability are some of the issues related to intensity estimation. In this Jupyter Notebook, we run a deep convolutional neural network architecture for categorizing hurricanes based on intensity using graphics\n",
        "processing unit.*\n",
        "\n",
        "*This Jupyter Notebook is a recreation of the Research Paper titled **\" Tropical Cyclone Intensity Estimation Using a Deep Convolutional Neural Network \"** by Ritesh Pradhan, Ramazan S. Aygun, Senior Member, IEEE, Manil Maskey, Member, IEEE, Rahul Ramachandran, Senior Member, IEEE, and Daniel J. Cecil *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PODWpl3fdUFG"
      },
      "source": [
        "## Motivation for the Problem \n",
        "\n",
        "\n",
        "\n",
        "##### Since hurricanes (or tropical cyclones) possess substantial threats and cause significant damage to lives and properties, studying the stages of a hurricane is essential to determine its impact. From a scientific perspective, determining an accurate TC intensity helps. \n",
        "\n",
        "\n",
        "* More accurate historical records of TCs, mainly if a technique can be consistently applied to older satellite imagery (i.e., intensity reanalysis)  \n",
        "\n",
        "* Providing consistent intensity estimates as current intensity estimates are made via a subjective algorithm (Dvorak technique) that is applied inconsistently in different forecast areas. Initial errors are too high, especially for weak and storms that are transitioning in structure.\n",
        "\n",
        "\n",
        "In this example, we use the Saffir-Simpson Hurricane Wind Scale (SSHWS) along with intensity categorization for tropical storm and tropical depression as tropical cyclone (TC) intensity categories.\n",
        "\n",
        "\n",
        "Since TC intensity is based on maximum wind speeds (MWS), estimating the TC intensity by just using image content is a challenging problem. Several techniques utilize satellite imagery for estimating tropical cyclone intensity using Dvorak and deviation-angle variance technique (DAVT) techniques.\n",
        "\n",
        "![alt_text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/cat+dvat.png?raw=1)\n",
        "\n",
        "### Dvorak Technique : \n",
        "\n",
        "The Dvorak technique is a method using enhanced Infrared and/or visible satellite imagery to quantitatively estimate the intensity of a tropical system. Cloud patterns in satellite imagery normally show an indication of cyclogenesis before the storm reaches tropical storm intensity. Indications of continued development and/or weakening can also be found in the cloud features. Using these features, the pattern formed by the clouds of a tropical cyclone, expected systematic development, and a series of rules, an intensity analysis and forecast can be made.\n",
        "\n",
        "The primary assumption of the Dvorak method is that cyclones with similar intensity tend to have a similar pattern.\n",
        "\n",
        "\n",
        "This Figure shows some development patterns used by the Dvorak technique. Once a pattern is detected over 24 hours, the features such as length and banding from the storm are further analyzed to reach a particular\n",
        "T-number. This relates tropical cloud structures to storm intensity. Nevertheless, this technique is not perfect and still suffers from subjective biases. Due to the inherent limitations of the empirical method used, it cannot determine subtropical cyclone intensity. Today, with the successful application of the Dvorak technique for more than 30 years along with some modifications and improvements, it is used worldwide for TC intensity estimation. The Advanced Dvorak Technique provides a nearly instantaneous estimate of TC intensity objectively. It removes a large amount of the subjectivity inherent in the process and produces errors similar to a human in most cases. But despite these attempts the errors can be high during the initial phase for weak storms that are transitioning in structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdjXsHmidUFH"
      },
      "source": [
        "**Now that we have understood the need for building a model to eliminate human errors, let us see how to approach the problem** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utzqD1quRI8K"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# PART 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlA_-lWHdUFI"
      },
      "source": [
        "# Approaching the Problem\n",
        "\n",
        "During this lab we will be making use of the following buckets to help us understand how a Machine Learning project should be planned and executed: \n",
        "\n",
        "1. **Data**: To start with any ML project we need data which is pre-processed and can be fed into the network.\n",
        "2. **Task**: There are many tasks present in ML, we need to make sure we understand and define the problem statement accurately.\n",
        "3. **Model**: We need to build our model, which is neither too deep or complex, thereby taking a lot of computational power or too small that it could not learn the important features.\n",
        "4. **Loss**: Out of the many _loss functions_ present, we need to carefully choose a _loss function_ which is suitable for the task we are about to carry out.\n",
        "5. **Learning**: As we mentioned in our last notebook, there are a variety of _optimisers_ each with their advantages and disadvantages. So here we choose an _optimiser_ which is suitable for our task and train our model using the set hyperparameters.\n",
        "6. **Evaluation**: This is a crucial step in the process to determine if our model has learnt the features properly by analysing how it performs when unseen data is given to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dnTN3zwdUFI"
      },
      "source": [
        "## Data\n",
        "\n",
        "##### The Image data will be annotated using the Text Data to create our dataset from which the model will learn to classify the type of cyclone.\n",
        "\n",
        "\n",
        "**Optional** : [Downloading Dataset](https://colab.sandbox.google.com/github/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/Downloading_Images.ipynb)\n",
        "\n",
        "**For the purposes of this lab, you do not need to download the images.**\n",
        "\n",
        "Example of images that will be fed into our model  : \n",
        "\n",
        "<table><tr>\n",
        "<td><img src=\"https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/example.jpg?raw=1\" alt=\"Drawing\" style=\"width: 320px;\"/></td>\n",
        "<td><img src=\"https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/example1.jpg?raw=1\" alt=\"Drawing\" style=\"width: 320px;\"/></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "*Source: https://www.nrlmry.navy.mil/*\n",
        "\n",
        "#### Each Image will be annotated to a category of Cyclone Intensity using the text data with the help of the following table :\n",
        "\n",
        "![alt text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/cat.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNFQyQ7IdUFJ"
      },
      "source": [
        "## Task \n",
        "\n",
        "There are a variety of tasks present in DL, and the task we are about to do is called Multi-class Classification. \n",
        "\n",
        "Here, multiple classes are present, and the model needs to classify the image into the correct class. \n",
        "\n",
        "The Classes here are the intensity of the tropical cyclone which will be estimated using the wind speeds of the cyclone: \n",
        "- NC ( No Category         , $\\leq 20$ knots)\n",
        "- TD ( Tropical Depression , $20-33$ knots)\n",
        "- TS ( Topical Storm       , $34-63$ knots)\n",
        "- H1 ( Category One        , $64-82$ knots)\n",
        "- H2 ( Category Two        , $83-95$ knots)\n",
        "- H3 ( Category Three      , $96-112$ knots)\n",
        "- H4 ( Category Four       , $113-136$ knots)\n",
        "- H5 ( Category Five       , $\\geq 137$ knots)\n",
        "\n",
        "##### Example of Multi-Class Classification is : \n",
        "<img src=\"https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/multi_class1.png?raw=1\" alt=\"Drawing\" style=\"width: 520px;\"/></td>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rar36_qcdUFJ"
      },
      "source": [
        "## Model and Loss \n",
        "\n",
        "#### We will be using the model given in the research paper : \n",
        "\n",
        "The Hyper-parameters in this model ( kernel size, number of hidden layers ) is tailor-made for this project. \n",
        "\n",
        "![alt text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/model.png?raw=1)\n",
        "\n",
        "\n",
        "### Loss Function :\n",
        "\n",
        "We have seen about three different Multi-class loss function, they are :\n",
        "- *Multi-Class Cross-Entropy Loss*\n",
        "- Sparse Multi-class Cross-Entropy Loss\n",
        "- Kullback Leibler Divergence Loss\n",
        "\n",
        "We will be using the Multi-class Cross-Entropy loss function for this classification. And one can use any of the three loss function for this task.\n",
        "\n",
        "##### Multi-Class Cross-Entropy loss : \n",
        "\n",
        "Before we understand how the loss value is calculated, let us understand how the outputs are produced.\n",
        "\n",
        "When we train our model , we convert all the categories to a array of 1's and 0's. Let us assume we have a model to predict if a given image is a cat or a dog, we will then start labelling all the outputs in our dataset and assign `cat = [ 1 , 0 ]` and `dog = [ 0 , 1 ] ` and we will then train our model based on it, our model will then predict the probabilities of the output of what it thinks it to be, let's say we give it an image of a cat, we get the output to be `[ 0.87 , 0.13]` which implies that the model is 87% confident that it is a cat,but this is still not good enough we then calculate the error in this case with the following equation. \n",
        "\n",
        "$$ Loss = J(w) = - \\frac{1}{N}\\sum_{n=0}^{N} \\left[ y_n \\log{\\hat{y_n}} + (1 - y_n) \\log{(1-\\hat{y_n})}  \\right] $$\n",
        "\n",
        "So ,if we try calculating the loss in our case from the equation , we will try calculating loss value for different values \n",
        "\n",
        "$ Loss = - \\frac{1}{2}\\left[ \\log{0.87} + (1 - 0) \\log{(1 - 0.13)} \\right] = -1 * \\log{0.87} = 0.060 $\n",
        "\n",
        "If our model trained over time and if the output would be `[ 0.90 , 0.10 ]` , then let's calculate our loss again.\n",
        "\n",
        "$ Loss = - \\frac{1}{2}\\left[ \\log{0.90} + (1 - 0) \\log{(1 - 0.10)} \\right] = -1 * \\log{0.90} = 0.045 $\n",
        "\n",
        "So ,now we notice our loss value to have reduced , so using these equations our model learns to understand how it performs and updates the weights to perform better. \n",
        "\n",
        "### Optimizer : \n",
        "\n",
        "In this model, we will be using SGD Optimizer ( Stochastic Gradient Descent ) Optimizer.\n",
        "\n",
        "##### Gradient Descent : \n",
        "\n",
        "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. \n",
        "\n",
        "Let us get an insight to understand how gradient descent works :\n",
        "\n",
        "Starting at the top of the mountain, we take our first step downhill in the direction specified by the negative gradient. Next, we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.\n",
        "\n",
        "<td><img src=\"https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/grad.jpg?raw=1\" alt=\"Drawing\" style=\"width: 420px;\"/></td>\n",
        "\n",
        "*Source: https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931*\n",
        "\n",
        "GD runs through all the samples in training set to do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use only one or subset of training sample from your training set to do the update for a parameter in a particular iteration. \n",
        "\n",
        "Using SGD will be faster because only one training sample is used and it starts improving itself right away from the first sample.\n",
        "\n",
        "\n",
        "## Training and Evaluation \n",
        "\n",
        "We will split our dataset into three different sets :\n",
        "\n",
        "- Training Set\n",
        "    - 72 % of the Dataset\n",
        "- Test Set \n",
        "    - 18 % of the Dataset\n",
        "- Validation Set \n",
        "    - 10% of the Dataset \n",
        "\n",
        "You are free to play around with these ratios in the next notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr4zglyKdUFJ"
      },
      "source": [
        "Summary of our approach : \n",
        "\n",
        "![alt_text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/now.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvdehYxFdUFK"
      },
      "source": [
        "# Working with the Image Dataset \n",
        "\n",
        "The images are downloaded in the 'gs://gpubootcamp_tcdata' Google Cloud Storage public bucket.\n",
        "\n",
        "### To work with the images, we need to understand the hierarchy of the Stored Images : \n",
        "\n",
        "The Hierarchy of this Dataset is similar to that of U.S.Naval Database for storing images, and this has been retained for storing the images as to easily scale / add more features to this.\n",
        "\n",
        "Let's now understand how it is arranged : \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "2VpUh8X7dUFK"
      },
      "source": [
        "<pre>\n",
        "tcdata                                  --> Root Folder\n",
        "└── tcdat                                --> First Nested Folder ( tcdat = tropical cyclone database )\n",
        "    ├── tc04                             --> Second Nested Folder ( tcxx = xx stand for yeah 20xx , tc04 = 2004) \n",
        "    │   └── ATL                          --> Third Nested Folder ( ATL -> Stands for Atlantic Cyclones)\n",
        "    │       ├── 01L.ALEX                 --> Fourth Nested Folder ( Name of the Cyclone)\n",
        "    │       │   └── ir                   --> Fifth Nested Folder ( Type of Image , We will be working with IR only)\n",
        "    │       │       └── geo              --> Sixth Nested Folder\n",
        "    │       │           └── 1km          --> Seventh Nested Folder ( Range of the Image ) \n",
        "                                         --> Images will be contained Here\n",
        "</pre>                                   \n",
        "So, Now we need to make sure we can recurse through this,   for example, we can show all the imags on the Google storage bucket for Hurricane Katrina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xrXmX75dUFK"
      },
      "source": [
        "!ls /content/gpubootcamp/tcdata/tcdat/tc05/ATL/12L.KATRINA/ir/geo/1km"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZLN7WGKdUFL"
      },
      "source": [
        "By a quick glance, we can understand the the Image Name has much information stored in it. \n",
        "\n",
        "Now, let us understand the format of the Data stored : \n",
        "\n",
        "*YYYYMMDD.HHMM*.Name of Satellite that captured the image and other relevant Data.jpg\n",
        "\n",
        "YYYYMMDD.HHMM  -> Year Month Date. Hours Minutes \n",
        "\n",
        "*Now we will be using the date time provided in the image name to annotate the type of category using the text data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a__k-JvndUFL"
      },
      "source": [
        "# Working with the Text Data \n",
        "\n",
        "We will be using the Best Track Data (HURDAT2) in the Atlantic Region for the Tropical Cyclone. From its [description](http://www.nhc.noaa.gov/data/#hurdat) on the NOAA's data web page\n",
        "\n",
        "<pre>\n",
        "Atlantic hurricane database (HURDAT2) 1851-2018</span> <br>\n",
        "This dataset was provided on 10 May 2019 to include the 2018 update to the best tracks.\n",
        "\n",
        "This dataset (<a href=\"/data/hurdat/hurdat2-format-atlantic.pdf\">known as Atlantic HURDAT2</a>) has\n",
        "a comma-delimited, text format with six-hourly information on the location,\n",
        "maximum winds, central pressure, and (beginning in 2004) size of all known tropical cyclones and subtropical cyclones.\n",
        "The original HURDAT database has been retired.</p>\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWPEl6sYdUFL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**( Optional - [Pre-processing the Data](Pre-Processing_Text_Data.ipynb))**\n",
        "\n",
        "This data follows a Modified CSV format because of which Pandas ( Python Library for Data Manipulation and Analysis ) can not get any useable data directly. Hence, we will be building our parser to pre-process this data and make it usable in the upcoming tasks.\n",
        "\n",
        "The steps followed in Notebook [Pre-Processing Text Data](Pre-Processing_Text_Data.ipynb) are :\n",
        "\n",
        "- Understanding the format of the data \n",
        "- Storing the cyclone in a dictionary\n",
        "- Converting the dictionary to a Dataframe\n",
        "- Restructuring the columns and making it readable\n",
        "- Replacing sentinel values and removing empty strings\n",
        "- Removing unwanted spaces and reindexing the Data frame\n",
        "- Save this Dataframe to a CSV File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4IGJBpZdUFL"
      },
      "source": [
        "## Let us have a look at the text data :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AOQzpbbdUFL"
      },
      "source": [
        "import pandas as pd\n",
        "atlantic_storms= pd.read_csv('gpubootcamp/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/atlantic.csv')\n",
        "atlantic_storms.tail(10)\n",
        "# We'll be using the Date and Time for Finding out the Velocity of the Cyclone \n",
        "# So we'll make it into a readable format\n",
        "\n",
        "atlantic_storms['date'] = pd.to_datetime(atlantic_storms['date'].astype(str))\n",
        "atlantic_storms['date'] = atlantic_storms.apply(lambda srs: srs['date'].replace(hour=int((\"%04d\" % srs['hours_minutes'])[:2]), minute=int((\"%04d\" % srs['hours_minutes'])[2:])), axis='columns')\n",
        "del atlantic_storms['hours_minutes']\n",
        "atlantic_storms.tail()\n",
        "# Let's now save our Text Data to Use it in the Upcoming Notebooks\n",
        "atlantic_storms.to_csv(\"atlantic_storms.csv\", encoding='utf-8')\n",
        "atlantic_storms.tail()['date']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZxguj6sRy6b"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# PART 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm8_yJyFS7Hs"
      },
      "source": [
        "## Understand the Model requirements\n",
        "\n",
        "### We have seen the model to which our image will be fed\n",
        "\n",
        "- The model described in the paper \n",
        "![alt text](https://raw.githubusercontent.com/gpuhackathons-org/gpubootcamp/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/model.png)\n",
        "\n",
        "We can see that the images need to be ( 232, 232, 3) in shape to be fed into our model.\n",
        "\n",
        "So, we will do the following steps before feeding the image into our model.\n",
        "\n",
        "- Step 1 : Resize Image from ( 1024, 1024 ,3) to ( 256 , 256 ,3 ) \n",
        "- Step 2 : Choose a random ( 232 , 232 , 3 ) patch from the ( 256 , 256 , 3 ) and feed into our model.\n",
        "\n",
        "**Alternate Approach** : We can modify the model's input shape to be ( 256 x 256 x 3 ) and train it on the scaled images, but we take a ( 232 x 232 x 3 ) random patch so that our model does not expect the cyclone to be in the center and learn to understanding the mapping even with the cyclones in the sides of the images.\n",
        "\n",
        "### Step 1 :\n",
        "Let's now start with Step 1 and understand all the resizing methods available to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E3GUQlDS6z-"
      },
      "source": [
        "import cv2\n",
        "#Read the Image Using cv2.imread()\n",
        "imgpath = 'gpubootcamp/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/'\n",
        "img = cv2.imread(imgpath + 'image_shape.jpg',1)\n",
        "#Changing the Color Spaces\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "# Print the Shape of the Image\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqh6nh6NS0Rs"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#Plot the image\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEQ1ZNX9T1yB"
      },
      "source": [
        "## Exploring different types of resizing options \n",
        "\n",
        "The Images can be resized in different ways. Some methods are as follows (as stated in OpenCV documentation) : \n",
        "\n",
        "<h3>Scaling</h3>\n",
        "<p>Scaling is just resizing of the image. OpenCV comes with a function <b>cv2.resize()</a></b> for this purpose. The size of the image can be specified manually, or you can specify the scaling factor. Different interpolation methods are used. Preferable interpolation methods are <b>cv2.INTER_AREA</b> for shrinking and <b>cv2.INTER_CUBIC</b> (slow) &amp; <b>cv2.INTER_LINEAR</b> for zooming. By default, interpolation method used is <b>cv2.INTER_LINEAR</b> for all resizing purposes.\n",
        "\n",
        "* cv2.INTER_AREA    ( Preferable for Shrinking ) \n",
        "* cv2.INTER_CUBIC   ( Preferable for Zooming but slow )\n",
        "* cv2.INTER_LINEAR  ( Preferable for Zooming and the default option "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3mBCTciUM6_"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9icGFG42DMP"
      },
      "source": [
        "def dummy():\n",
        "  pass\n",
        "\n",
        "def load_dataset(augment_fn = dummy):\n",
        "    import os\n",
        "    import cv2\n",
        "    from datetime import datetime\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from scipy import interpolate \n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    #Variables to be used later\n",
        "    filenames = []\n",
        "    labels =[]\n",
        "    i = 0  \n",
        "    #Read CSV file using Pandas\n",
        "    df = pd.read_csv('atlantic_storms.csv')\n",
        "\n",
        "    dir ='/content/gpubootcamp/tcdata/tcdat'\n",
        "    a = os.listdir(dir)\n",
        "\n",
        "    file_path = \"Dataset/Aug/\"\n",
        "    directory = os.path.dirname(file_path)\n",
        " \n",
        "    aug = 0 \n",
        "    for j in a :\n",
        "        c = os.listdir(dir+'/'+j)\n",
        "        for k in c :\n",
        "            d = os.listdir(dir+'/'+j+'/'+k)\n",
        "            for l in d :\n",
        "                print(l)\n",
        "                start_year= '20'+j[2:]+ '-01-01'\n",
        "                end_year= '20'+j[2:]+ '-12-31'\n",
        "                cyc_name = l[4:]\n",
        "                mask = (df['date'] > start_year ) & (df['date'] <= end_year ) & ( df['name'] == cyc_name )\n",
        "                cyc_pd = df.loc[mask]\n",
        "                first = (datetime.strptime(cyc_pd['date'].iloc[0], \"%Y-%m-%d %H:%M:%S\"))\n",
        "                last = (datetime.strptime(cyc_pd['date'].iloc[-1], \"%Y-%m-%d %H:%M:%S\"))\n",
        "                text_time=[]\n",
        "                text_vel=[]\n",
        "                for q in range(len(cyc_pd['date'])):\n",
        "                    text_vel.append(cyc_pd['maximum_sustained_wind_knots'].iloc[q])\n",
        "                    text_time.append((datetime.strptime(cyc_pd['date'].iloc[q],\"%Y-%m-%d %H:%M:%S\")-first).total_seconds())\n",
        "                func = interpolate.splrep(text_time,text_vel)\n",
        "                e = os.listdir(dir+'/'+j+'/'+k+'/'+l+'/ir/geo/1km')\n",
        "                e.sort()\n",
        "                for m in e :\n",
        "                    try :\n",
        "                        time=(datetime.strptime(m[:13], \"%Y%m%d.%H%M\"))\n",
        "                        name = dir+'/'+j+'/'+k+'/'+l+'/ir/geo/1km/'+m\n",
        "                        if(time>first and time < last):\n",
        "                            val = int(interpolate.splev((time-first).total_seconds(),func))\n",
        "                            filenames.append(name)\n",
        "                            if val <=20 :\n",
        "                                labels.append(0)\n",
        "                            elif val>20 and val <=33 :\n",
        "                                labels.append(1)\n",
        "                            elif val>33 and val <=63 :\n",
        "                                labels.append(2)\n",
        "                            elif val>63 and val <=82 :\n",
        "                                labels.append(3)\n",
        "                            elif val>82 and val <=95 :\n",
        "                                labels.append(4)\n",
        "                            elif val>95 and val <=112 :\n",
        "                                labels.append(5)\n",
        "                            elif val>112 and val <=136 :\n",
        "                                labels.append(6)\n",
        "                            elif val>136 :\n",
        "                                labels.append(7)\n",
        "                            i = augment_fn(name,labels[-1],filenames,labels,i)\n",
        "                    except :\n",
        "                        pass\n",
        "    print('')\n",
        "    print(len(filenames)) \n",
        "     # Shuffle The Data\n",
        "    import random\n",
        "    # Zip Images with Appropriate Labels before Shuffling\n",
        "    c = list(zip(filenames, labels))\n",
        "    random.shuffle(c)\n",
        "    #Unzip the Data Post Shuffling\n",
        "    filenames, labels = zip(*c)\n",
        "    filenames = list(filenames)\n",
        "    labels = list(labels)\n",
        "    return filenames,labels\n",
        "filenames,labels = load_dataset()\n",
        "print(filenames)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYtLGvokdUFO"
      },
      "source": [
        "# Annotating our dataset \n",
        "\n",
        "Let us start by taking an example of Katrina Hurricane from 2005 and scaling it for all the Cyclones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwS2QhrLdUFO"
      },
      "source": [
        "import pandas as pd\n",
        "# Read the CSV we saved earlier\n",
        "df = pd.read_csv('atlantic_storms.csv')\n",
        "# Create a Mask to Filter our Katrina Cyclone (2005)\n",
        "mask = (df['date'] > '2005-01-01') & (df['date'] <= '2006-01-01') & ( df['name'] == 'KATRINA')\n",
        "# Apply the Mask to the Original Data Frame and Extract the new Dataframe\n",
        "new_df = df.loc[mask]\n",
        "new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaEkjb1ldUFM"
      },
      "source": [
        "Here, we notice the text data to be sampled once every six hours because of which we will be forced to use interpolation techniques to find the velocity at any particular instant. \n",
        "\n",
        "Now determining the velocity at any time instance with this interpolated data is going to be deviated from the truth value, but we know that a class has a range of velocity so the probability that our interpolated class being correct is more realstic as compared to the former.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OaAu_PDdUFP"
      },
      "source": [
        "**We can observe, the images are taken once 30 minutes, but the text data is available once every 6 hours.  So we will be interpolating the text data to fit the curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqu-J-7zdUFP"
      },
      "source": [
        "#Get list of Dates and Velocity from the New Dataframe\n",
        "date_list = new_df['date'].tolist()\n",
        "velocity_list = new_df['maximum_sustained_wind_knots'].tolist()\n",
        "print(date_list[:5])\n",
        "type(date_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qigbGvyNdUFP"
      },
      "source": [
        "The Dates are in STR Format which we will be converting now to datetime format to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIUwqcu6dUFP"
      },
      "source": [
        "from datetime import datetime\n",
        "# Get and Convert to Datetime format for the First Last recorded time of Text Data.\n",
        "first = (datetime.strptime(date_list[0], \"%Y-%m-%d %H:%M:%S\"))\n",
        "last = (datetime.strptime(date_list[-1], \"%Y-%m-%d %H:%M:%S\"))\n",
        "print(first)\n",
        "type(first)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t334WANVdUFP"
      },
      "source": [
        "#Changes the list from Convert everything to seconds from the first image to interpolate the data\n",
        "for i in range(len(date_list)):\n",
        "    date_list[i]=( (datetime.strptime(date_list[i], \"%Y-%m-%d %H:%M:%S\")) - first ).total_seconds()\n",
        "print(date_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddSSc0K9dUFQ"
      },
      "source": [
        "# Interpolate using the Scipy Library Funciton\n",
        "from scipy import interpolate\n",
        "func = interpolate.splrep(date_list,velocity_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqsD06P1dUFQ"
      },
      "source": [
        "#Getting List of Katrina Images \n",
        "import os\n",
        "e = os.listdir('/content/gpubootcamp/tcdata/tcdat/tc05/ATL/12L.KATRINA/ir/geo/1km')\n",
        "# Sort images by time\n",
        "e.sort()\n",
        "x=[]\n",
        "y=[]\n",
        "for m in e :\n",
        "    try :\n",
        "        #Strip the Time Data from image and convert it the a datetime type.\n",
        "        time_img=(datetime.strptime(m[:13], \"%Y%m%d.%H%M\"))\n",
        "        # If the Image is taken between the available text data\n",
        "        if(time_img>=first and time_img <= last):\n",
        "            # Get Interpolated Value for that time and Save It \n",
        "            value = int(interpolate.splev((time_img-first).total_seconds(),func))\n",
        "            x.append((time_img-first).total_seconds())\n",
        "            y.append(value)\n",
        "    except :\n",
        "       pass \n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT-B_R9bdUFQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot All the Saved Data Points\n",
        "f = plt.figure(figsize=(24,10))\n",
        "ax = f.add_subplot(121)\n",
        "ax2 = f.add_subplot(122)\n",
        "ax.title.set_text('Datapoints frm csv file')\n",
        "ax2.title.set_text('Interpolated from CSV file to images')\n",
        "ax.plot(date_list,velocity_list,'-o')\n",
        "ax2.plot(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMLBaf69dUFQ"
      },
      "source": [
        "**Now we have interpolated and found relevant velocity for all images between the recorded text timeframe. Let us now use it for training our Model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9s-iptMdUFQ"
      },
      "source": [
        "# Wrapping Things Up :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKXh1ycRdUFQ"
      },
      "source": [
        "### Preparing the Dataset\n",
        "\n",
        "#####  All the above modules are joined together and make It into a single function to load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glcC1pW5dUFR"
      },
      "source": [
        "def make_test_set(filenames,labels,val=0.1):\n",
        "    classes = 8\n",
        "    j=0\n",
        "    val_filenames=[]\n",
        "    val_labels=[]\n",
        "    new = [int(val*len(filenames)/classes)]*classes\n",
        "    print(new)\n",
        "    try:\n",
        "        for i in range(len(filenames)):\n",
        "            if(new[labels[i]]>0):\n",
        "                val_filenames.append(filenames[i])\n",
        "                val_labels.append(labels[i])\n",
        "                new[labels[i]] = new[labels[i]]-1\n",
        "                del filenames[i]\n",
        "                del labels[i]\n",
        "    except :\n",
        "        pass\n",
        "    \n",
        "     # Shuffle The Data\n",
        "    import random\n",
        "    # Zip Images with Appropriate Labels before Shuffling\n",
        "    c = list(zip(val_filenames, val_labels))\n",
        "    random.shuffle(c)\n",
        "    #Unzip the Data Post Shuffling\n",
        "    val_filenames, val_labels = zip(*c)\n",
        "    val_filenames = list(val_filenames)\n",
        "    val_labels = list(val_labels)\n",
        "    from collections import Counter\n",
        "    print(Counter(labels))\n",
        "    return val_filenames,val_labels  \n",
        "val_filenames , val_labels = make_test_set(filenames,labels,val=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APD79N7idUFR"
      },
      "source": [
        "## Understand our dataset :\n",
        "\n",
        "We can see the following lines from the Output : \n",
        "\n",
        "`[344, 344, 344, 344, 344, 344, 344, 344]` and `{2: 7936, 3: 5339, 1: 3803, 4: 2934, 5: 2336, 6: 2178, 7: 204, 0: 100}`\n",
        "\n",
        "This is the distribution of our validation set and training set over it's classes. \n",
        "\n",
        "For the validation set we use *Stratified Validation* set so that our validation set nearly respresent the whole class. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDjrs8bhdUFR"
      },
      "source": [
        "#Make train test set\n",
        "test = 0.2\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(filenames, labels, test_size=test, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpePWS5DdUFR"
      },
      "source": [
        "## One-Hot Encoding \n",
        "\n",
        "`y_train` is a list containing data from 0-7 such as [ 2,4,5,....] but our Model Needs an Input of Array for Each Output as as 1D vector  :\n",
        "\n",
        "2 --- > [ 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0] \n",
        "\n",
        "4 --- > [ 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0] \n",
        "\n",
        "\n",
        "This is encoded as such because keeping the other values 0 is necessary for the model to find the model Loss and use backpropagation for making it learn the _Weight Matrix_.\n",
        "\n",
        "The below given image is an example of One-Hot Encoding :\n",
        "\n",
        "![alt text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/one_hot.jfif?raw=1)\n",
        "\n",
        "Reference : [What is One Hot Encoding and How to Do It](https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsPjnIRkdUFR"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_train = tf.one_hot(y_train,depth=8)\n",
        "y_test = tf.one_hot(y_test,depth=8)\n",
        "val_labels = tf.one_hot(val_labels,depth=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7fXG3-ldUFR"
      },
      "source": [
        "def parse_function(filename, label):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "\n",
        "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "\n",
        "    #This will convert to float values in [0, 1]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    #Resize Image \n",
        "    image = tf.image.resize(image, [232, 232])\n",
        "    \n",
        "    return image, label\n",
        "def make_dataset(train_in,test_in,val_in):\n",
        "    import tensorflow as tf\n",
        "    train = tf.data.Dataset.from_tensor_slices((train_in[0], train_in[1]))\n",
        "    train = train.shuffle(len(train_in[0]))\n",
        "    train = train.map(parse_function,num_parallel_calls=8)\n",
        "    train = train.batch(train_in[2])\n",
        "    train = train.prefetch(1)\n",
        "    test = tf.data.Dataset.from_tensor_slices((test_in[0], test_in[1]))\n",
        "    test = test.shuffle(len(test_in[0]))\n",
        "    test = test.map(parse_function, num_parallel_calls=8)\n",
        "    test = test.batch(test_in[2])\n",
        "    test = test.prefetch(1)\n",
        "    val = tf.data.Dataset.from_tensor_slices((val_in[0],val_in[1] ))\n",
        "    val = val.map(parse_function, num_parallel_calls=8)\n",
        "    val = val.batch(val_in[2])\n",
        "    val = val.prefetch(1)\n",
        "    return train,test,val\n",
        "\n",
        "train,test,val = make_dataset((x_train,y_train,128),(x_test,y_test,32),(val_filenames,val_labels,32))\n",
        "print(test)\n",
        "print(train)\n",
        "print(val)\n",
        "print(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6kJUwICdUFS"
      },
      "source": [
        "## Defining our Model\n",
        "\n",
        "![alt_text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/model.png?raw=1)\n",
        "\n",
        "We will be Implementing this model in Keras using the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56VmRnxvdUFS"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "tf.random.set_seed(1337)\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten ,Dropout, MaxPooling2D\n",
        "from tensorflow.keras import backend as K \n",
        "\n",
        "#Reset Graphs and Create Sequential model\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "#Convolution Layers\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=10,strides=3, activation='relu', input_shape=(232,232,3)))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
        "model.add(Conv2D(256, kernel_size=5,strides=1,activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
        "model.add(Conv2D(288, kernel_size=3,strides=1,padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=1))\n",
        "model.add(Conv2D(272, kernel_size=3,strides=1,padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, kernel_size=3,strides=1,activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Linear Layers \n",
        "\n",
        "model.add(Dense(3584,activation='relu'))\n",
        "model.add(Dense(2048,activation='relu'))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "# Print Model Summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImITNvX3dUFS"
      },
      "source": [
        "### Compiling and Training our Model\n",
        "\n",
        "We will be using the following : \n",
        "\n",
        "- Optimizer : SGD ( Stochastic Gradient Descent ) with parameters mentioned in the research paper.\n",
        "    - Learning Rate : 0.001\n",
        "    - Momentum : 0.9\n",
        "- Loss Function : Categorical Cross Entropy ( Used in Multi-class classification ) \n",
        "- Metrics : We will be using two metrics to determine how our model performs \n",
        "    - Accuracy  : Number of Predictions correct / Total number of Predictions\n",
        "    - Top -2 Accuracy : Top-2 accuracy means that any of your model 2 highest probability answers must match the expected answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cySKJFWCdUFS"
      },
      "source": [
        "import functools\n",
        "\n",
        "# Include Top-2 Accuracy Metrics \n",
        "top2_acc = functools.partial(tensorflow.keras.metrics.top_k_categorical_accuracy, k=2)\n",
        "top2_acc.__name__ = 'top2_acc'\n",
        "\n",
        "#Define Number of Epochs\n",
        "epochs = 4\n",
        "\n",
        "#But Training our model from scratch will take a long time\n",
        "#So we will load a partially trained model to speedup the process \n",
        "model.load_weights(\"trained_16.h5\")\n",
        "\n",
        "# Optimizer\n",
        "sgd = tensorflow.keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9)\n",
        "\n",
        "\n",
        "#Compile Model with Loss Function , Optimizer and Metrics\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, \n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy',top2_acc])\n",
        "\n",
        "# Train the Model \n",
        "trained_model = model.fit(train,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=val)\n",
        "\n",
        "# Test Model Aganist Validation Set\n",
        "score = model.evaluate(test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHXeXdb6dUFS"
      },
      "source": [
        "### Visualisations\n",
        "\n",
        "Let us now visualise how our model perfromed during the training process : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xya9y-0ndUFS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f = plt.figure(figsize=(15,5))\n",
        "ax = f.add_subplot(121)\n",
        "ax.plot(trained_model.history['accuracy'])\n",
        "ax.plot(trained_model.history['val_accuracy'])\n",
        "ax.set_title('Model Accuracy')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.legend(['Train', 'Val'])\n",
        "\n",
        "ax2 = f.add_subplot(122)\n",
        "ax2.plot(trained_model.history['loss'])\n",
        "ax2.plot(trained_model.history['val_loss'])\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.legend(['Train', 'Val'],loc= 'upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fi9VqEudUFT"
      },
      "source": [
        "## Confusion Matrix :\n",
        "\n",
        "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. \n",
        "\n",
        "Here , the rows display the predicted class and the columns are the truth value of the classes.From this we can estimate how our model performs over different classes which would in turn help us determine how our data should be fed into our model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "60VPWuWldUFT"
      },
      "source": [
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "#Plotting a heatmap using the confusion matrix\n",
        "pred = model.predict(val)\n",
        "p = np.argmax(pred, axis=1)\n",
        "y_valid = np.argmax(val_labels, axis=1, out=None)\n",
        "results = confusion_matrix(y_valid, p) \n",
        "classes=['NC','TD','TC','H1','H3','H3','H4','H5']\n",
        "df_cm = pd.DataFrame(results, index = [i for i in classes], columns = [i for i in classes])\n",
        "plt.figure(figsize = (15,15))\n",
        "\n",
        "sn.heatmap(df_cm, annot=True, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i5gKRBhdUFT"
      },
      "source": [
        "**Congralutions on running your first model. Now In the next notebook , let us try to understand the drawbacks of this model and make it better :**\n",
        "\n",
        "\n",
        "We can notice that the validation accuracy is lesser than the training accuracy. This is because the model is not properly Regularized and the possible reasons are : \n",
        "\n",
        "**Not enough data-points / Imbalanced classes**\n",
        "\n",
        "Using different techniques we will be regulating and normalising the model in the upcoming notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjvHRsL6U-eV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# PART 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y603YJnmdUFU"
      },
      "source": [
        "# Understanding the drawbacks\n",
        "\n",
        "```python3\n",
        "Simply put, a machine learning model is only as good as the data it is fed with\n",
        "```\n",
        "We have achieved an accuracy nearly of ~85% running with 4 epochs. Now we will try to increase the accuracy by taking a closer look at the dataset and images. We can observe the following from our previous notebook : "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "zhnWEdbRdUFU"
      },
      "source": [
        "NC :441\n",
        "TD :4033\n",
        "TC :7948\n",
        "H1 :5340\n",
        "H2 :3150\n",
        "H3 :2441\n",
        "H4 :2114\n",
        "H5 :390"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n6AYahWdUFU"
      },
      "source": [
        "First thing that we will notice from the category count is that the number of images per category is very un-uniform with ratios of TC: H5 **greater than 1:20**, This imbalance can bias the vision of our CNN model because predicting wrong on the minority class wouldn't impact the model a lot as the class contribution is less than 5% of the dataset.\n",
        "\n",
        "The same can be shown also by the heatmap we obtained in the previous notebook : Notice Most of Classes with higher data was predicted correctly and the minority class was more mis-predicted than the other classes \n",
        "![alt_text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/heatmap.png?raw=1)\n",
        "\n",
        "\n",
        "Let us see now how we solve that problem using data augmentation : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx4Gn4vSdUFU"
      },
      "source": [
        "## Working out the solution\n",
        "\n",
        "## Data Augmentation \n",
        "\n",
        "To decrease the un-uniformity, we will be flipping and rotating images to compensate for the lack of data for class with less samples: \n",
        "\n",
        "![alt text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/augment.png?raw=1)\n",
        "\n",
        "\n",
        "We will be using OpenCV for Flipping and Image Rotations. \n",
        "\n",
        "``` python\n",
        "cv2.flip(img,0)\n",
        "cv2.flip(img,1)\n",
        "cv2.warpAffine(img, cv2.getRotationMatrix2D(center, 90, 1.0), (h, w))\n",
        "cv2.warpAffine(img, cv2.getRotationMatrix2D(center, 180, 1.0), (w, h))\n",
        "cv2.warpAffine(img, cv2.getRotationMatrix2D(center, 270, 1.0), (h, w))\n",
        "```\n",
        "\n",
        "There are other ways to counter data imbalance such as Class weightage, Oversampling, SMOTE etc.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeO6BeqQdUFU"
      },
      "source": [
        "# Training the Model with Data Augmentation \n",
        "\n",
        "\n",
        "We create a new function called `augmentation(name,category,filenames,labels,i)` and here we add more samples to Category which have imbalanced data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb22CXHt9RPq"
      },
      "source": [
        "import os\n",
        "file_path = \"/content/Dataset/Aug/\"\n",
        "directory = os.path.dirname(file_path)\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"Dataset\")\n",
        "  os.mkdir(\"Dataset/Aug\")\n",
        "  print(\"Directory Created\")\n",
        "except:\n",
        "  print(\"Directory Exists\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukzdZ7tpdUFV"
      },
      "source": [
        "import sys\n",
        "import cv2\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "def load_image(name,interpolation = cv2.INTER_AREA):\n",
        "    img=cv2.imread(name,1)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    inter_area = cv2.resize(img,(256,256),interpolation=interpolation)\n",
        "    start_pt= np.random.randint(24,size=2)\n",
        "    end_pt = start_pt + [232,232]\n",
        "    img = inter_area[start_pt[0]:end_pt[0],start_pt[1]:end_pt[1]]\n",
        "    return img\n",
        "\n",
        "# Define the Augmentation Function \n",
        "def augmentation(name,category,filenames,labels,i):\n",
        "    # Important Constants\n",
        "    file_path = \"Dataset/Aug/\"\n",
        "    images = []\n",
        "    (h, w) = (232,232)\n",
        "    center = (w / 2, h / 2)\n",
        "    angle90 = 90\n",
        "    angle180 = 180\n",
        "    angle270 = 270\n",
        "    scale = 1.0\n",
        "    img = load_image(name , interpolation = cv2.INTER_LINEAR)\n",
        "    \n",
        "    \n",
        "    if category == 0 :\n",
        "        images.append(cv2.flip(img,0))\n",
        "    elif category == 1 :\n",
        "        pass\n",
        "    elif category == 2 :\n",
        "        pass\n",
        "    elif category == 3 :\n",
        "        pass\n",
        "    elif category == 4 :\n",
        "        pass\n",
        "    elif category == 5 :\n",
        "        pass\n",
        "    elif category == 6 :\n",
        "        pass\n",
        "    elif category == 7 :\n",
        "        images.append(cv2.flip(img,0))\n",
        "        \n",
        "        \n",
        "    for j in range(len(images)):\n",
        "        cv2.imwrite(file_path+str(i+j)+'.jpeg',images[j])\n",
        "        filenames.append(file_path+str(i+j)+'.jpeg')\n",
        "        labels.append(category)\n",
        "    i = i + len(images)\n",
        "    return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pisOynZQdUFV"
      },
      "source": [
        "**We pass this function to our `load_dataset()` function to generate these augmentations. **\n",
        "\n",
        "Kindly wait for a couple of minutes while augments the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6GTd6TPkdUFV"
      },
      "source": [
        "filenames,labels = load_dataset(augment_fn = augmentation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwuW1L11dUFV"
      },
      "source": [
        "# Set the Size of the Validation set\n",
        "val_filenames , val_labels = make_test_set(filenames,labels,val=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfBDWPiTdUFV"
      },
      "source": [
        "#Make train test set \n",
        "test = 0.1\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(filenames, labels, test_size=test, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoHIXUrydUFW"
      },
      "source": [
        "import tensorflow as tf\n",
        "y_train = tf.one_hot(y_train,depth=8)\n",
        "y_test = tf.one_hot(y_test,depth=8)\n",
        "val_labels = tf.one_hot(val_labels,depth=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3UZeuODdUFW"
      },
      "source": [
        "# Make Dataset compatible with Tensorflow Data Pipelining.\n",
        "train,test,val = make_dataset((x_train,y_train,128),(x_test,y_test,32),(val_filenames,val_labels,32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL8acrrQdUFW"
      },
      "source": [
        "# The model described in the paper :\n",
        "\n",
        "Now we will be using the model described in the paper to evaluate it's accuracy on the new dataset.\n",
        "\n",
        "![alt_text](https://github.com/jrossthomson/gpubootcamp/blob/master/hpc_ai/ai_science_climate/English/python/jupyter_notebook/Tropical_Cyclone_Intensity_Estimation/images/model.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5fta4fYdUFW"
      },
      "source": [
        "import numpy as np\n",
        "tf.random.set_seed(1337)\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten ,Dropout, MaxPooling2D\n",
        "from tensorflow.keras import backend as K \n",
        "\n",
        "#Reset Graphs and Create Sequential model\n",
        "\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "#Convolution Layers\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=10,strides=3, activation='relu', input_shape=(232,232,3)))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
        "model.add(Conv2D(256, kernel_size=5,strides=1,activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
        "model.add(Conv2D(288, kernel_size=3,strides=1,padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=1))\n",
        "model.add(Conv2D(272, kernel_size=3,strides=1,padding='same',activation='relu'))\n",
        "model.add(Conv2D(256, kernel_size=3,strides=1,activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3),strides=2))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Linear Layers \n",
        "\n",
        "model.add(Dense(3584,activation='relu'))\n",
        "model.add(Dense(2048,activation='relu'))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "# Print Model Summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ept8LelAdUFW"
      },
      "source": [
        "import functools\n",
        "# Include Top-2 Accuracy Metrics \n",
        "top2_acc = functools.partial(tensorflow.keras.metrics.top_k_categorical_accuracy, k=2)\n",
        "top2_acc.__name__ = 'top2_acc'\n",
        "\n",
        "#Define Number of Epochs\n",
        "epochs = 4\n",
        "\n",
        "#But Training our model from scratch will take a long time\n",
        "#So we will load a partially trained model to speedup the process \n",
        "model.load_weights(\"trained_16.h5\")\n",
        "\n",
        "# Optimizer\n",
        "sgd = tensorflow.keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9)\n",
        "\n",
        "\n",
        "#Compile Model with Loss Function , Optimizer and Metrics\n",
        "model.compile(loss=tensorflow.keras.losses.categorical_crossentropy, \n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy',top2_acc])\n",
        "\n",
        "# Train the Model \n",
        "trained_model = model.fit(train,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=val)\n",
        "\n",
        "# Test Model Aganist Validation Set\n",
        "score = model.evaluate(test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj5XoYJjdUFW"
      },
      "source": [
        "### Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g3igCSrdUFX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f = plt.figure(figsize=(15,5))\n",
        "ax = f.add_subplot(121)\n",
        "ax.plot(trained_model.history['accuracy'])\n",
        "ax.plot(trained_model.history['val_accuracy'])\n",
        "ax.set_title('Model Accuracy')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.legend(['Train', 'Val'])\n",
        "\n",
        "ax2 = f.add_subplot(122)\n",
        "ax2.plot(trained_model.history['loss'])\n",
        "ax2.plot(trained_model.history['val_loss'])\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.legend(['Train', 'Val'],loc= 'upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pwfs3mKdUFX"
      },
      "source": [
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "#Plotting a heatmap using the confusion matrix\n",
        "\n",
        "pred = model.predict(val)\n",
        "p = np.argmax(pred, axis=1)\n",
        "y_valid = np.argmax(val_labels, axis=1, out=None)\n",
        "results = confusion_matrix(y_valid, p) \n",
        "classes=['NC','TD','TC','H1','H3','H3','H4','H5']\n",
        "df_cm = pd.DataFrame(results, index = [i for i in classes], columns = [i for i in classes])\n",
        "plt.figure(figsize = (15,15))\n",
        "\n",
        "sn.heatmap(df_cm, annot=True, cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR_XpyZJdUFX"
      },
      "source": [
        "Let us now save our Model and the trained weights for future usage :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts090xDQdUFX"
      },
      "source": [
        "#Save Our Model \n",
        "model.save('cyc_pred.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWLFqirEVNLQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# PART 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGkA1IiBdUFY"
      },
      "source": [
        "<H1>Competition</H1>\n",
        "\n",
        "In this exercise participant need to tune and work on improving overall acuracy of our model. \n",
        "\n",
        "To help you get started by pointing out some obvious ways in which you can make the model more efficient. \n",
        "\n",
        "- Epochs  \n",
        "- Batch Size \n",
        "- Optimizers : We have used SGD as a optimizer. Participant can try applying other optimizer and test to obtain quick convergence.\n",
        "- Data Augmentation : Remember, we mentioned we have an imbalanced dataset. You could try differnet augmentation techniques for the minority classes.\n",
        "- Model : If you have exploited all the bbove methods to improve your model, you can change the model by adding more Layers to it and see if that improves that accuracy.\n",
        "\n",
        "Note, before you start tweaking and training your model ,it would be worthwhile to refer to these to see how they affect your model : \n",
        "\n",
        "[Epochs impact on Overfitting](https://datascience.stackexchange.com/questions/27561/can-the-number-of-epochs-influence-overfitting ) \n",
        "\n",
        "\n",
        "[Effect of Batch Size on Training Dynamics](https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e)\n",
        "\n",
        "[Introduction to Optimizers](https://algorithmia.com/blog/introduction-to-optimizers)"
      ]
    }
  ]
}